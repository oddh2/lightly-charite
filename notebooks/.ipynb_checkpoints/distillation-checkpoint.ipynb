{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightly\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def make_label_unlabeled_split(**kwargs):\n",
    "    #split U and L from labeled dataset\n",
    "    \n",
    "    data_path = kwargs.get('data_path')\n",
    "    saving_unlabeled_path = kwargs.get('U_path')\n",
    "    saving_labeled_path = kwargs.get('L_path')\n",
    "    sample = kwargs.get('L_size',0.01)\n",
    "\n",
    "    saving_unlabeled_path.mkdir(parents=True, exist_ok=True)\n",
    "    saving_labeled_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    folder_list = [folder for folder in data_path.iterdir() if folder.is_dir()]\n",
    "    for folder in folder_list:\n",
    "\n",
    "        dest_cat_path = saving_labeled_path.joinpath(folder.name)\n",
    "        dest_cat_path.mkdir(parents = True,exist_ok=True)\n",
    "\n",
    "        img_list = [filepath for filepath in folder.iterdir()]\n",
    "\n",
    "        all_indexes = np.arange(len(img_list))\n",
    "        label_idx = np.random.randint(len(img_list),size = int(sample*len(img_list)))\n",
    "        unlabel_idx = np.array([idx for idx in all_indexes if idx not in label_idx])\n",
    "        label_imgs = [img_list[i] for i in label_idx]\n",
    "        unlabel_imgs = [img_list[i] for i in unlabel_idx]\n",
    "\n",
    "        for img in label_imgs:\n",
    "            dest_label_img_path = dest_cat_path.joinpath(img.name)\n",
    "            copyfile(img,dest_label_img_path)\n",
    "\n",
    "        for img in unlabel_imgs:\n",
    "            dest_unlabel_img_path = saving_unlabeled_path.joinpath(img.name)\n",
    "            copyfile(img,dest_unlabel_img_path)\n",
    "            \n",
    "\n",
    "class UnlabeledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, main_dir, transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        self.all_imgs = os.listdir(main_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.all_imgs[idx])\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image\n",
    "    \n",
    "def class_distribution(dataset,class_index_dict):\n",
    "    count_dict = {}\n",
    "    for img,label in dataset:\n",
    "        if class_index_dict[label] not in count_dict:\n",
    "            count_dict.update({class_index_dict[label]:0})\n",
    "        else:\n",
    "            count_dict[class_index_dict[label]] += 1\n",
    "            \n",
    "    return count_dict\n",
    "    \n",
    "def sample_dataset(dataset,sample):\n",
    "    sample_idx = np.random.randint(len(unlabeled_dataset),size = int(sample*len(unlabeled_dataset)))\n",
    "    return torch.utils.data.Subset(dataset, sample_idx)\n",
    "    \n",
    "def load_simcrl(simclr_results_path,n_categories,model_size = 18):\n",
    "    \n",
    "    #load config\n",
    "    conf_path = simclr_results_path.joinpath('conf.json')\n",
    "    with open(conf_path,'r') as f:\n",
    "        conf = json.load(f)\n",
    "\n",
    "    #load model\n",
    "    model_path = simclr_results_path.joinpath('checkpoint.pth')\n",
    "\n",
    "    num_ftrs = conf['num_ftrs']\n",
    "\n",
    "    resnet = lightly.models.ResNetGenerator('resnet-'+str(model_size))\n",
    "    last_conv_channels = list(resnet.children())[-1].in_features\n",
    "    backbone = nn.Sequential(\n",
    "        *list(resnet.children())[:-1],\n",
    "        nn.Conv2d(last_conv_channels, num_ftrs, 1),\n",
    "        nn.AdaptiveAvgPool2d(1)\n",
    "    )\n",
    "\n",
    "    model = lightly.models.SimCLR(backbone, num_ftrs=num_ftrs)\n",
    "\n",
    "    encoder = lightly.embedding.SelfSupervisedEmbedding(\n",
    "        model,\n",
    "        None,\n",
    "        None,\n",
    "        None\n",
    "    )\n",
    "\n",
    "    encoder.model.load_state_dict(torch.load(model_path))\n",
    "    teacher = Teacher(encoder.model,num_ftrs,n_categories).to(device)\n",
    "    return teacher\n",
    "    \n",
    "    \n",
    "def evaluate(model,testloader,loss_function):\n",
    "  val_loss = 0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  ground_truth_list = []\n",
    "  predictions_list =  []\n",
    "  for image,label in testloader:\n",
    "      image, label = image.to(device), label.to(device)\n",
    "      outputs = model(image)\n",
    "      probabilities, predicted = torch.max(outputs.data, 1)\n",
    "      val_loss += loss_function(outputs, label.long()).item()\n",
    "      total += label.size(0)\n",
    "      correct += (predicted == label).sum().item()\n",
    "      ground_truth_list += list(label.cpu())\n",
    "      predictions_list += list(predicted.cpu())\n",
    "\n",
    "  acc = sklearn.metrics.accuracy_score(ground_truth_list,predictions_list)\n",
    "  f1 = sklearn.metrics.f1_score(ground_truth_list,predictions_list,average = 'macro')\n",
    "  precision = sklearn.metrics.precision_score(ground_truth_list,predictions_list,average = 'macro')\n",
    "  recall = sklearn.metrics.recall_score(ground_truth_list,predictions_list,average = 'macro')\n",
    "  print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')\n",
    "\n",
    "  metrics_dict = {'val_loss':val_loss,'acc':acc,'f1':f1,'precision':precision,'recall':recall}\n",
    "\n",
    "  return metrics_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big-self supervised models are strong semi-supervised radiologists\n",
    "\n",
    "\n",
    "Inspired by [this paper](https://arxiv.org/pdf/2006.10029.pdf)\n",
    "\n",
    "\n",
    "We have a small subset of labeled data $L$ and a large pool of unlabeled data $U$. The goal is to make the most out of $U$ for training a classifier for solving the task on $L$.\n",
    "\n",
    "The procedure has three steps: \n",
    "\n",
    "* Pretrain a big SimCLR model on $U$\n",
    "* Fine-tune on $L$\n",
    "* Use the resulting model as a teacher for a smaller model, which is trained on the predictions of the teacher model on $U$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split $L$ into a training and testing set using stratified sampling for getting splits with the same proportions of labels. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results path\n",
    "results_path = Path('/projects/self_supervised/results/sortifier_distillation')\n",
    "results_path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "\n",
    "sample_unlabeled = 1.0\n",
    "test_size = 0.8\n",
    "\n",
    "data_path = Path('/projects/self_supervised/data/sortifier')\n",
    "unlabeled_path = Path('/projects/self_supervised/data/sortifier_unlabeled')\n",
    "labeled_path = Path('/projects/self_supervised/data/sortifier_labeled')\n",
    "\n",
    "# #not required if you already splitted the data\n",
    "# make_label_unlabeled_split(\n",
    "#     data_path = data_path,\n",
    "#     U_path = unlabeled_path,\n",
    "#     L_path = labeled_path,\n",
    "#     L_size = 0.02\n",
    "# )\n",
    "\n",
    "\n",
    "input_size = 64\n",
    "batch_size = 16\n",
    "num_workers = 2\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "    mean=lightly.data.collate.imagenet_normalize['mean'],\n",
    "    std=lightly.data.collate.imagenet_normalize['std'],\n",
    "    )\n",
    "])\n",
    "\n",
    "labeled_dataset = datasets.ImageFolder(root=labeled_path, transform=transform)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size)\n",
    "\n",
    "img_paths = [item[0] for item in labeled_dataset.imgs]\n",
    "labels = [item[1] for item in labeled_dataset.imgs]\n",
    "idx_train, idx_test = next(sss.split(img_paths,labels))\n",
    "\n",
    "trainset = torch.utils.data.Subset(labeled_dataset, idx_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "testset = torch.utils.data.Subset(labeled_dataset, idx_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "class_index_dict = {v:k for k,v in testset.dataset.class_to_idx.items()}\n",
    "\n",
    "#load unlabeled data\n",
    "unlabeled_dataset = UnlabeledDataset(unlabeled_path,transform = transform)\n",
    "unlabeled_dataset = sample_dataset(unlabeled_dataset,sample_unlabeled)\n",
    "unlabeledloader = torch.utils.data.DataLoader(\n",
    "    unlabeled_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 107\n",
      "testing 431\n",
      "unlabeled 26640\n"
     ]
    }
   ],
   "source": [
    "print('training',len(trainset))\n",
    "#print('training',class_distribution(trainset,class_index_dict))\n",
    "print('testing',len(testset))\n",
    "#print('testing',class_distribution(testset,class_index_dict))\n",
    "print('unlabeled',len(unlabeled_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla student model (resnet18)\n",
    "\n",
    "We train on $L$ a resnet18 as a baseline.\n",
    "\n",
    "We define the softmax-like function $P(y|x_{i})=\\frac{exp(f(x_{i})[y]/\\tau}{\\sum_{y'} exp(f(x_{i})[y']/\\tau}$, where $\\tau$ is a temperature parameter and $f$ the model. \n",
    "\n",
    "We will use cross-entropy defined as $-\\sum_{(x_{i},y_{i})} \\left[ log P(y_{i}|x_{i})\\right]$ as the loss function for training on $L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(nn.Module):\n",
    "    def __init__(self,output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = models.resnet18(pretrained=True)\n",
    "        self.net.fc = nn.Linear(self.net.fc.in_features, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "    \n",
    "def P(x,tau = 1.0):\n",
    "  return torch.exp(x/tau)/(torch.exp(x/tau).sum())\n",
    "\n",
    "class CrossEntropyLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,n_categories):\n",
    "        super(CrossEntropyLoss,self).__init__()\n",
    "        self.n_classes = n_categories\n",
    "\n",
    "    def forward(self, prediction, label):\n",
    "      label = torch.nn.functional.one_hot(label,num_classes=n_categories)\n",
    "      loss = -label*torch.log(P(prediction))\n",
    "      return loss.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_categories = len([cat for cat in labeled_path.iterdir()])\n",
    "crossent_loss = CrossEntropyLoss(n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.832 f1:0.604 precision:0.691 recall:0.619\n",
      "acc:0.875 f1:0.689 precision:0.796 recall:0.691\n",
      "acc:0.839 f1:0.656 precision:0.724 recall:0.676\n",
      "acc:0.882 f1:0.667 precision:0.752 recall:0.681\n",
      "acc:0.923 f1:0.755 precision:0.848 recall:0.731\n",
      "acc:0.909 f1:0.765 precision:0.897 recall:0.723\n",
      "acc:0.916 f1:0.715 precision:0.846 recall:0.698\n",
      "acc:0.933 f1:0.776 precision:0.904 recall:0.746\n",
      "acc:0.928 f1:0.766 precision:0.868 recall:0.734\n",
      "acc:0.911 f1:0.729 precision:0.860 recall:0.684\n",
      "acc:0.925 f1:0.736 precision:0.834 recall:0.715\n",
      "acc:0.942 f1:0.773 precision:0.897 recall:0.745\n",
      "acc:0.947 f1:0.811 precision:0.961 recall:0.780\n",
      "acc:0.940 f1:0.783 precision:0.907 recall:0.756\n",
      "acc:0.916 f1:0.736 precision:0.895 recall:0.693\n",
      "acc:0.923 f1:0.754 precision:0.896 recall:0.719\n",
      "acc:0.923 f1:0.732 precision:0.874 recall:0.703\n",
      "\n",
      "Best metrics:\n",
      "acc:0.947 f1:0.811 precision:0.961 recall:0.780\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train the student model on labeled data\n",
    "\n",
    "student = Student(n_categories).to(device)\n",
    "optimizer = optim.Adam(student.parameters(), lr=0.0001)\n",
    "\n",
    "patience = 3\n",
    "count = 0\n",
    "best_loss = 1e9\n",
    "for epoch in range(40):\n",
    "  for image,label in trainloader:\n",
    "      image, label = image.to(device), label.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      loss = crossent_loss(student(image), label)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "        \n",
    "  metrics_dict = evaluate(student,testloader,crossent_loss)\n",
    "  val_loss = metrics_dict['val_loss']\n",
    "  if val_loss < best_loss:        \n",
    "        best_loss = val_loss\n",
    "        best_metrics = metrics_dict\n",
    "        \n",
    "        count = 0\n",
    "  else:\n",
    "    count += 1\n",
    "  if count > patience:\n",
    "    break\n",
    "        \n",
    "        \n",
    "print('\\nBest metrics:')\n",
    "acc = best_metrics['acc']\n",
    "f1 = best_metrics['f1']\n",
    "recall = best_metrics['recall']\n",
    "precision = best_metrics['precision']\n",
    "print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big teacher (SimCLR)\n",
    "\n",
    "\n",
    "We fine-tune on $L$ a SimCLR model with backbone resnet50 pretrained on $U$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(nn.Module):\n",
    "    def __init__(self, model,num_ftrs,output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.freeze = False\n",
    "        self.net = model\n",
    "        self.fc1 = nn.Linear(num_ftrs, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.freeze:\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net.backbone(x).squeeze()\n",
    "                y_hat = self.fc1(y_hat)\n",
    "                y_hat = self.relu(y_hat)\n",
    "                y_hat = self.fc2(y_hat)\n",
    "                return y_hat\n",
    "        else:\n",
    "            y_hat = self.net.backbone(x).squeeze()\n",
    "            y_hat = self.fc1(y_hat)\n",
    "            y_hat = self.relu(y_hat)\n",
    "            y_hat = self.fc2(y_hat)\n",
    "            return y_hat\n",
    "            \n",
    "    def freeze_weights(self):\n",
    "      for p in self.net.parameters():\n",
    "          p.requires_grad = False\n",
    "      self.freeze = True\n",
    "      return self\n",
    "\n",
    "    def unfreeze_weights(self):\n",
    "      for p in self.net.parameters():\n",
    "          p.requires_grad = True\n",
    "      self.freeze = False\n",
    "      return self\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/miniconda3/envs/sort_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.911 f1:0.690 precision:0.698 recall:0.690\n",
      "acc:0.938 f1:0.743 precision:0.963 recall:0.739\n",
      "acc:0.945 f1:0.768 precision:0.967 recall:0.749\n",
      "acc:0.909 f1:0.693 precision:0.951 recall:0.646\n",
      "acc:0.935 f1:0.796 precision:0.928 recall:0.747\n",
      "acc:0.945 f1:0.825 precision:0.907 recall:0.793\n",
      "acc:0.947 f1:0.803 precision:0.968 recall:0.773\n",
      "acc:0.945 f1:0.774 precision:0.965 recall:0.768\n",
      "acc:0.942 f1:0.783 precision:0.867 recall:0.771\n",
      "acc:0.964 f1:0.881 precision:0.952 recall:0.853\n",
      "acc:0.954 f1:0.861 precision:0.923 recall:0.836\n",
      "acc:0.957 f1:0.838 precision:0.905 recall:0.820\n",
      "acc:0.959 f1:0.856 precision:0.921 recall:0.832\n",
      "acc:0.952 f1:0.819 precision:0.894 recall:0.805\n",
      "\n",
      "Best metrics:\n",
      "acc:0.964 f1:0.881 precision:0.952 recall:0.853\n"
     ]
    }
   ],
   "source": [
    "#finetune simclr\n",
    "simclr_results_path = Path('/projects/self_supervised/results/sortifier_unlabeled')\n",
    "\n",
    "teacher = load_simcrl(simclr_results_path,n_categories,model_size = 50)\n",
    "optimizer = optim.Adam(teacher.parameters(), lr=0.0001)\n",
    "\n",
    "patience = 3\n",
    "count = 0\n",
    "best_loss = 1e9\n",
    "for epoch in range(40):\n",
    "  for image,label in trainloader:\n",
    "      image, label = image.to(device), label.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      loss = crossent_loss(teacher(image), label.long())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "  metrics_dict = evaluate(teacher,testloader,crossent_loss)\n",
    "  val_loss = metrics_dict['val_loss']\n",
    "  if val_loss < best_loss:\n",
    "        torch.save(teacher.state_dict(),results_path.joinpath('teacher.pth'))\n",
    "        best_loss = val_loss\n",
    "        best_metrics = metrics_dict\n",
    "        count = 0\n",
    "        \n",
    "  else:\n",
    "    count += 1\n",
    "  if count > patience:\n",
    "    break\n",
    "\n",
    "print('\\nBest metrics:')\n",
    "acc = best_metrics['acc']\n",
    "f1 = best_metrics['f1']\n",
    "recall = best_metrics['recall']\n",
    "precision = best_metrics['precision']\n",
    "print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model distillation\n",
    "\n",
    "The fine-tuned model often yields better performance than the vanilla model. Now that we used $L$ for finetuning the teacher model, we can use $U$ again for transfering the knowledge from the teacher to the student\n",
    "\n",
    "The procedure is as follows:\n",
    "* sample data from $U$\n",
    "* predict with the teacher\n",
    "* use the labels as targets for training the student\n",
    "\n",
    "This algorithm is expected to yield an even better model. Notice that the resulting model would be a resnet18 with even better performance than a resnet50 pretrained on $U$ and fine-tuned on $L$.\n",
    "\n",
    "We will use a distillation loss, which takes the output probabilities of the teacher model as the target for the student model.\n",
    "\n",
    "$$ -\\sum_{x_{i}} \\left[ \\sum_{y} P^{T}(y|x_{i};\\tau) log P^{S}(y|x_{i};\\tau) \\right] $$\n",
    "\n",
    "With this loss function the student doesn't only see the one-vs-zero encoding used in supervised learning, but a probability distribution as a target. This could help the student model to learn nuances of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistilLoss,self).__init__()\n",
    "\n",
    "    def forward(self, t_output, s_output):\n",
    "      loss = -P(t_output)*torch.log(P(s_output))\n",
    "      return loss.sum().sum()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.942 f1:0.745 precision:0.842 recall:0.743\n",
      "acc:0.947 f1:0.725 precision:0.719 recall:0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/miniconda3/envs/sort_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.952 f1:0.796 precision:0.974 recall:0.776\n",
      "acc:0.950 f1:0.836 precision:0.871 recall:0.816\n",
      "acc:0.957 f1:0.818 precision:0.972 recall:0.797\n"
     ]
    }
   ],
   "source": [
    "#init student\n",
    "student = Student(n_categories).to(device)\n",
    "\n",
    "#load teacher checkpoint\n",
    "teacher = load_simcrl(simclr_results_path,n_categories,model_size = 50)\n",
    "teacher.load_state_dict(torch.load(results_path.joinpath('teacher.pth')))\n",
    "teacher = teacher.freeze_weights()\n",
    "teacher.eval()\n",
    "\n",
    "distill_loss = DistilLoss()\n",
    "optimizer = optim.Adam(student.parameters(), lr=0.00001)\n",
    "\n",
    "patience = 5\n",
    "count = 0\n",
    "best_loss = 1e9\n",
    "for epoch in range(40):\n",
    "  for image in unlabeledloader:\n",
    "    image = image.to(device)\n",
    "    loss = distill_loss(teacher(image),student(image))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  metrics_dict = evaluate(student,testloader,crossent_loss)\n",
    "  val_loss = metrics_dict['val_loss']\n",
    "  if val_loss < best_loss:        \n",
    "        best_loss = val_loss\n",
    "        best_metrics = metrics_dict\n",
    "        count = 0\n",
    "  else:\n",
    "    count += 1\n",
    "  if count > patience:\n",
    "    break\n",
    "\n",
    "print('\\nBest metrics:')\n",
    "acc = best_metrics['acc']\n",
    "f1 = best_metrics['f1']\n",
    "recall = best_metrics['recall']\n",
    "precision = best_metrics['precision']\n",
    "print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
