{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightly\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def make_label_unlabeled_split(**kwargs):\n",
    "    #split U and L from labeled dataset\n",
    "    \n",
    "    data_path = kwargs.get('data_path')\n",
    "    saving_unlabeled_path = kwargs.get('U_path')\n",
    "    saving_labeled_path = kwargs.get('L_path')\n",
    "    sample = kwargs.get('L_size',0.01)\n",
    "\n",
    "    saving_unlabeled_path.mkdir(parents=True, exist_ok=True)\n",
    "    saving_labeled_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    folder_list = [folder for folder in data_path.iterdir() if folder.is_dir()]\n",
    "    for folder in folder_list:\n",
    "\n",
    "        dest_cat_path = saving_labeled_path.joinpath(folder.name)\n",
    "        dest_cat_path.mkdir(parents = True,exist_ok=True)\n",
    "\n",
    "        img_list = [filepath for filepath in folder.iterdir()]\n",
    "\n",
    "        all_indexes = np.arange(len(img_list))\n",
    "        label_idx = np.random.randint(len(img_list),size = int(sample*len(img_list)))\n",
    "        unlabel_idx = np.array([idx for idx in all_indexes if idx not in label_idx])\n",
    "        label_imgs = [img_list[i] for i in label_idx]\n",
    "        unlabel_imgs = [img_list[i] for i in unlabel_idx]\n",
    "\n",
    "        for img in label_imgs:\n",
    "            dest_label_img_path = dest_cat_path.joinpath(img.name)\n",
    "            copyfile(img,dest_label_img_path)\n",
    "\n",
    "        for img in unlabel_imgs:\n",
    "            dest_unlabel_img_path = saving_unlabeled_path.joinpath(img.name)\n",
    "            copyfile(img,dest_unlabel_img_path)\n",
    "            \n",
    "\n",
    "class UnlabeledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, main_dir, transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        self.all_imgs = os.listdir(main_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.all_imgs[idx])\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image\n",
    "    \n",
    "def class_distribution(dataset,class_index_dict):\n",
    "    count_dict = {}\n",
    "    for img,label in dataset:\n",
    "        if class_index_dict[label] not in count_dict:\n",
    "            count_dict.update({class_index_dict[label]:0})\n",
    "        else:\n",
    "            count_dict[class_index_dict[label]] += 1\n",
    "            \n",
    "    return count_dict\n",
    "    \n",
    "def sample_dataset(dataset,sample):\n",
    "    sample_idx = np.random.randint(len(unlabeled_dataset),size = int(sample*len(unlabeled_dataset)))\n",
    "    return torch.utils.data.Subset(dataset, sample_idx)\n",
    "    \n",
    "def load_simcrl(simclr_results_path,n_categories,model_size = 18):\n",
    "    \n",
    "    #load config\n",
    "    conf_path = simclr_results_path.joinpath('conf.json')\n",
    "    with open(conf_path,'r') as f:\n",
    "        conf = json.load(f)\n",
    "\n",
    "    #load model\n",
    "    model_path = simclr_results_path.joinpath('checkpoint.pth')\n",
    "\n",
    "    num_ftrs = conf['num_ftrs']\n",
    "\n",
    "    resnet = lightly.models.ResNetGenerator('resnet-'+str(model_size))\n",
    "    last_conv_channels = list(resnet.children())[-1].in_features\n",
    "    backbone = nn.Sequential(\n",
    "        *list(resnet.children())[:-1],\n",
    "        nn.Conv2d(last_conv_channels, num_ftrs, 1),\n",
    "        nn.AdaptiveAvgPool2d(1)\n",
    "    )\n",
    "\n",
    "    model = lightly.models.SimCLR(backbone, num_ftrs=num_ftrs)\n",
    "\n",
    "    encoder = lightly.embedding.SelfSupervisedEmbedding(\n",
    "        model,\n",
    "        None,\n",
    "        None,\n",
    "        None\n",
    "    )\n",
    "\n",
    "    encoder.model.load_state_dict(torch.load(model_path))\n",
    "    teacher = Teacher(encoder.model,num_ftrs,n_categories).to(device)\n",
    "    return teacher\n",
    "    \n",
    "    \n",
    "def evaluate(model,testloader,loss_function):\n",
    "  val_loss = 0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  ground_truth_list = []\n",
    "  predictions_list =  []\n",
    "  for image,label in testloader:\n",
    "      image, label = image.to(device), label.to(device)\n",
    "      outputs = model(image)\n",
    "      probabilities, predicted = torch.max(outputs.data, 1)\n",
    "      val_loss += loss_function(outputs, label.long()).item()\n",
    "      total += label.size(0)\n",
    "      correct += (predicted == label).sum().item()\n",
    "      ground_truth_list += list(label.cpu())\n",
    "      predictions_list += list(predicted.cpu())\n",
    "\n",
    "  acc = sklearn.metrics.accuracy_score(ground_truth_list,predictions_list)\n",
    "  f1 = sklearn.metrics.f1_score(ground_truth_list,predictions_list,average = 'macro')\n",
    "  precision = sklearn.metrics.precision_score(ground_truth_list,predictions_list,average = 'macro')\n",
    "  recall = sklearn.metrics.recall_score(ground_truth_list,predictions_list,average = 'macro')\n",
    "  print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')\n",
    "\n",
    "  metrics_dict = {'val_loss':val_loss,'acc':acc,'f1':f1,'precision':precision,'recall':recall}\n",
    "\n",
    "  return metrics_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big-self supervised models are strong semi-supervised radiologists\n",
    "\n",
    "\n",
    "Inspired by [this paper](https://arxiv.org/pdf/2006.10029.pdf)\n",
    "\n",
    "\n",
    "We have a small subset of labeled data $L$ and a large pool of unlabeled data $U$. The goal is to make the most out of $U$ for training a classifier for solving the task on $L$.\n",
    "\n",
    "The procedure has three steps: \n",
    "\n",
    "* Pretrain a big SimCLR model on $U$\n",
    "* Fine-tune on $L$\n",
    "* Use the resulting model as a teacher for a smaller model, which is trained on the predictions of the teacher model on $U$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split $L$ into a training and testing set using stratified sampling for getting splits with the same proportions of labels. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results path\n",
    "results_path = Path('/projects/self_supervised/results/sortifier_distillation')\n",
    "results_path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "\n",
    "sample_unlabeled = 1.0\n",
    "test_size = 0.8\n",
    "\n",
    "data_path = Path('/projects/self_supervised/data/sortifier')\n",
    "unlabeled_path = Path('/projects/self_supervised/data/sortifier_unlabeled')\n",
    "labeled_path = Path('/projects/self_supervised/data/sortifier_labeled')\n",
    "\n",
    "# #not required if you already splitted the data\n",
    "# make_label_unlabeled_split(\n",
    "#     data_path = data_path,\n",
    "#     U_path = unlabeled_path,\n",
    "#     L_path = labeled_path,\n",
    "#     L_size = 0.02\n",
    "# )\n",
    "\n",
    "\n",
    "input_size = 64\n",
    "batch_size = 16\n",
    "num_workers = 2\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "    mean=lightly.data.collate.imagenet_normalize['mean'],\n",
    "    std=lightly.data.collate.imagenet_normalize['std'],\n",
    "    )\n",
    "])\n",
    "\n",
    "labeled_dataset = datasets.ImageFolder(root=labeled_path, transform=transform)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size)\n",
    "\n",
    "img_paths = [item[0] for item in labeled_dataset.imgs]\n",
    "labels = [item[1] for item in labeled_dataset.imgs]\n",
    "idx_train, idx_test = next(sss.split(img_paths,labels))\n",
    "\n",
    "trainset = torch.utils.data.Subset(labeled_dataset, idx_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "testset = torch.utils.data.Subset(labeled_dataset, idx_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "class_index_dict = {v:k for k,v in testset.dataset.class_to_idx.items()}\n",
    "\n",
    "#load unlabeled data\n",
    "unlabeled_dataset = UnlabeledDataset(unlabeled_path,transform = transform)\n",
    "unlabeled_dataset = sample_dataset(unlabeled_dataset,sample_unlabeled)\n",
    "unlabeledloader = torch.utils.data.DataLoader(\n",
    "    unlabeled_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 107\n",
      "testing 431\n",
      "unlabeled 26640\n"
     ]
    }
   ],
   "source": [
    "print('training',len(trainset))\n",
    "#print('training',class_distribution(trainset,class_index_dict))\n",
    "print('testing',len(testset))\n",
    "#print('testing',class_distribution(testset,class_index_dict))\n",
    "print('unlabeled',len(unlabeled_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla student model (resnet18)\n",
    "\n",
    "We train on $L$ a resnet18 as a baseline.\n",
    "\n",
    "We define the softmax-like function $P(y|x_{i})=\\frac{exp(f(x_{i})[y]/\\tau}{\\sum_{y'} exp(f(x_{i})[y']/\\tau}$, where $\\tau$ is a temperature parameter and $f$ the model. \n",
    "\n",
    "We will use cross-entropy defined as $-\\sum_{(x_{i},y_{i})} \\left[ log P(y_{i}|x_{i})\\right]$ as the loss function for training on $L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(nn.Module):\n",
    "    def __init__(self,output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = models.resnet18(pretrained=True)\n",
    "        self.net.fc = nn.Linear(self.net.fc.in_features, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "    \n",
    "def P(x,tau = 1.0):\n",
    "  return torch.exp(x/tau)/(torch.exp(x/tau).sum())\n",
    "\n",
    "class CrossEntropyLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,n_categories):\n",
    "        super(CrossEntropyLoss,self).__init__()\n",
    "        self.n_classes = n_categories\n",
    "\n",
    "    def forward(self, prediction, label):\n",
    "      label = torch.nn.functional.one_hot(label,num_classes=n_categories)\n",
    "      loss = -label*torch.log(P(prediction))\n",
    "      return loss.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_categories = len([cat for cat in labeled_path.iterdir()])\n",
    "crossent_loss = CrossEntropyLoss(n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.690 f1:0.589 precision:0.613 recall:0.690\n",
      "acc:0.873 f1:0.733 precision:0.718 recall:0.753\n",
      "acc:0.897 f1:0.769 precision:0.761 recall:0.777\n",
      "acc:0.885 f1:0.735 precision:0.748 recall:0.724\n",
      "acc:0.899 f1:0.749 precision:0.784 recall:0.728\n",
      "acc:0.897 f1:0.723 precision:0.818 recall:0.689\n",
      "acc:0.921 f1:0.744 precision:0.864 recall:0.705\n",
      "acc:0.916 f1:0.752 precision:0.854 recall:0.717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fcabbd48f7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mmetrics_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrossent_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-947971371768>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, testloader, loss_function)\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0mground_truth_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mpredictions_list\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sort_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sort_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sort_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sort_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sort_env/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sort_env/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sort_env/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sort_env/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sort_env/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#train the student model on labeled data\n",
    "\n",
    "student = Student(n_categories).to(device)\n",
    "optimizer = optim.Adam(student.parameters(), lr=0.0001)\n",
    "\n",
    "patience = 3\n",
    "count = 0\n",
    "best_loss = 1e9\n",
    "for epoch in range(40):\n",
    "  for image,label in trainloader:\n",
    "      image, label = image.to(device), label.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      loss = crossent_loss(student(image), label)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "        \n",
    "  metrics_dict = evaluate(student,testloader,crossent_loss)\n",
    "  val_loss = metrics_dict['val_loss']\n",
    "  if val_loss < best_loss:        \n",
    "        best_loss = val_loss\n",
    "        best_metrics = metrics_dict\n",
    "        \n",
    "        count = 0\n",
    "  else:\n",
    "    count += 1\n",
    "  if count > patience:\n",
    "    break\n",
    "        \n",
    "        \n",
    "print('\\nBest metrics:')\n",
    "acc = best_metrics['acc']\n",
    "f1 = best_metrics['f1']\n",
    "recall = best_metrics['recall']\n",
    "precision = best_metrics['precision']\n",
    "print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big teacher (SimCLR)\n",
    "\n",
    "\n",
    "We fine-tune on $L$ a SimCLR model with backbone resnet50 pretrained on $U$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(nn.Module):\n",
    "    def __init__(self, model,num_ftrs,output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.freeze = False\n",
    "        self.net = model\n",
    "        self.fc1 = nn.Linear(num_ftrs, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.freeze:\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net.backbone(x).squeeze()\n",
    "                y_hat = self.fc1(y_hat)\n",
    "                y_hat = self.relu(y_hat)\n",
    "                y_hat = self.fc2(y_hat)\n",
    "                return y_hat\n",
    "        else:\n",
    "            y_hat = self.net.backbone(x).squeeze()\n",
    "            y_hat = self.fc1(y_hat)\n",
    "            y_hat = self.relu(y_hat)\n",
    "            y_hat = self.fc2(y_hat)\n",
    "            return y_hat\n",
    "            \n",
    "    def freeze_weights(self):\n",
    "      for p in self.net.parameters():\n",
    "          p.requires_grad = False\n",
    "      self.freeze = True\n",
    "      return self\n",
    "\n",
    "    def unfreeze_weights(self):\n",
    "      for p in self.net.parameters():\n",
    "          p.requires_grad = True\n",
    "      self.freeze = False\n",
    "      return self\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetune simclr\n",
    "simclr_results_path = Path('/projects/self_supervised/results/sortifier_unlabeled')\n",
    "\n",
    "teacher = load_simcrl(simclr_results_path,n_categories,model_size = 50)\n",
    "optimizer = optim.Adam(teacher.parameters(), lr=0.0001)\n",
    "\n",
    "patience = 3\n",
    "count = 0\n",
    "best_loss = 1e9\n",
    "for epoch in range(40):\n",
    "  for image,label in trainloader:\n",
    "      image, label = image.to(device), label.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      loss = crossent_loss(teacher(image), label.long())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "  metrics_dict = evaluate(teacher,testloader,crossent_loss)\n",
    "  val_loss = metrics_dict['val_loss']\n",
    "  if val_loss < best_loss:\n",
    "        torch.save(teacher.state_dict(),results_path.joinpath('teacher.pth'))\n",
    "        best_loss = val_loss\n",
    "        best_metrics = metrics_dict\n",
    "        count = 0\n",
    "        \n",
    "  else:\n",
    "    count += 1\n",
    "  if count > patience:\n",
    "    break\n",
    "\n",
    "print('\\nBest metrics:')\n",
    "acc = best_metrics['acc']\n",
    "f1 = best_metrics['f1']\n",
    "recall = best_metrics['recall']\n",
    "precision = best_metrics['precision']\n",
    "print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model distillation\n",
    "\n",
    "The fine-tuned model often yields better performance than the vanilla model. Now that we used $L$ for finetuning the teacher model, we can use $U$ again for transfering the knowledge from the teacher to the student\n",
    "\n",
    "The procedure is as follows:\n",
    "* sample data from $U$\n",
    "* predict with the teacher\n",
    "* use the labels as targets for training the student\n",
    "\n",
    "This algorithm is expected to yield an even better model. Notice that the resulting model would be a resnet18 with even better performance than a resnet50 pretrained on $U$ and fine-tuned on $L$.\n",
    "\n",
    "We will use a distillation loss, which takes the output probabilities of the teacher model as the target for the student model.\n",
    "\n",
    "$$ -\\sum_{x_{i}} \\left[ \\sum_{y} P^{T}(y|x_{i};\\tau) log P^{S}(y|x_{i};\\tau) \\right] $$\n",
    "\n",
    "With this loss function the student doesn't only see the one-vs-zero encoding used in supervised learning, but a probability distribution as a target. This could help the student model to learn nuances of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistilLoss,self).__init__()\n",
    "\n",
    "    def forward(self, t_output, s_output):\n",
    "      loss = -P(t_output)*torch.log(P(s_output))\n",
    "      return loss.sum().sum()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init student\n",
    "student = Student(n_categories).to(device)\n",
    "\n",
    "#load teacher checkpoint\n",
    "teacher = load_simcrl(simclr_results_path,n_categories,model_size = 50)\n",
    "teacher.load_state_dict(torch.load(results_path.joinpath('teacher.pth')))\n",
    "teacher = teacher.freeze_weights()\n",
    "teacher.eval()\n",
    "\n",
    "distill_loss = DistilLoss()\n",
    "optimizer = optim.Adam(student.parameters(), lr=0.00005)\n",
    "\n",
    "patience = 5\n",
    "count = 0\n",
    "best_loss = 1e9\n",
    "for epoch in range(40):\n",
    "  for image in unlabeledloader:\n",
    "    image = image.to(device)\n",
    "    loss = distill_loss(teacher(image),student(image))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  metrics_dict = evaluate(student,testloader,crossent_loss)\n",
    "  val_loss = metrics_dict['val_loss']\n",
    "  if val_loss < best_loss:        \n",
    "        best_loss = val_loss\n",
    "        best_metrics = metrics_dict\n",
    "        count = 0\n",
    "  else:\n",
    "    count += 1\n",
    "  if count > patience:\n",
    "    break\n",
    "\n",
    "print('\\nBest metrics:')\n",
    "acc = best_metrics['acc']\n",
    "f1 = best_metrics['f1']\n",
    "recall = best_metrics['recall']\n",
    "precision = best_metrics['precision']\n",
    "print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
