{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightly\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "            \n",
    "def make_train_test_split(**kwargs):\n",
    "    test_size = kwargs.get('test_size')\n",
    "    labeled_path = kwargs.get('labeled_path')\n",
    "    train_path = kwargs.get('train_path')\n",
    "    test_path = kwargs.get('test_path')\n",
    "    \n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size)\n",
    "\n",
    "    img_paths = []\n",
    "    labels = []\n",
    "    for label in labeled_path.iterdir():\n",
    "        for img in labeled_path.joinpath(label).iterdir():\n",
    "            img_paths.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "    idx_train, idx_test = next(sss.split(img_paths,labels))\n",
    "\n",
    "    train_imgs = [img_paths[i] for i in idx_train]\n",
    "    train_labels = [labels[i] for i in idx_train]\n",
    "\n",
    "    test_imgs = [img_paths[i] for i in idx_test]\n",
    "    test_labels = [labels[i] for i in idx_test]\n",
    "    \n",
    "    for img,label in zip(train_imgs,train_labels):        \n",
    "        dest_cat_path = train_path.joinpath(label.name)\n",
    "        dest_cat_path.mkdir(parents=True, exist_ok=True)\n",
    "        copyfile(img,dest_cat_path.joinpath(img.name))\n",
    "        \n",
    "    for img,label in zip(test_imgs,test_labels):        \n",
    "        dest_cat_path = test_path.joinpath(label.name)\n",
    "        dest_cat_path.mkdir(parents=True, exist_ok=True)\n",
    "        copyfile(img,dest_cat_path.joinpath(img.name))\n",
    "            \n",
    "    \n",
    "def class_distribution(dataset,class_index_dict):\n",
    "    count_dict = {}\n",
    "    for img,label in dataset:\n",
    "        if class_index_dict[label] not in count_dict:\n",
    "            count_dict.update({class_index_dict[label]:0})\n",
    "        else:\n",
    "            count_dict[class_index_dict[label]] += 1\n",
    "            \n",
    "    return count_dict\n",
    "    \n",
    "def sample_dataset(dataset,sample):\n",
    "    sample_idx = np.random.randint(len(unlabeled_dataset),size = int(sample*len(unlabeled_dataset)))\n",
    "    return torch.utils.data.Subset(dataset, sample_idx)\n",
    "    \n",
    "def load_simcrl(simclr_results_path,n_categories,model_size = 18):\n",
    "    \n",
    "    #load config\n",
    "    conf_path = simclr_results_path.joinpath('conf.json')\n",
    "    with open(conf_path,'r') as f:\n",
    "        conf = json.load(f)\n",
    "\n",
    "    #load model\n",
    "    model_path = simclr_results_path.joinpath('checkpoint.pth')\n",
    "\n",
    "    num_ftrs = conf['num_ftrs']\n",
    "    \n",
    "    model_name = conf['model_name']\n",
    "\n",
    "    resnet = lightly.models.ResNetGenerator('resnet-'+str(model_size))\n",
    "    last_conv_channels = list(resnet.children())[-1].in_features\n",
    "    backbone = nn.Sequential(\n",
    "        *list(resnet.children())[:-1],\n",
    "        nn.Conv2d(last_conv_channels, num_ftrs, 1),\n",
    "        nn.AdaptiveAvgPool2d(1)\n",
    "    )\n",
    "\n",
    "    if model_name == 'simclr':\n",
    "        model = lightly.models.SimCLR(backbone, num_ftrs=num_ftrs)\n",
    "    elif model_name == 'moco':\n",
    "        model = lightly.models.MoCo(backbone, num_ftrs=num_ftrs, m=0.99, batch_shuffle=True)\n",
    "        \n",
    "\n",
    "    encoder = lightly.embedding.SelfSupervisedEmbedding(\n",
    "        model,\n",
    "        None,\n",
    "        None,\n",
    "        None\n",
    "    )\n",
    "\n",
    "    encoder.model.load_state_dict(torch.load(model_path))\n",
    "    teacher = Teacher(encoder.model,num_ftrs,n_categories).to(device)\n",
    "    return teacher\n",
    "    \n",
    "    \n",
    "def evaluate(model,testloader,loss_function):\n",
    "  val_loss = 0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  ground_truth_list = []\n",
    "  predictions_list =  []\n",
    "  for image,label in testloader:\n",
    "      image, label = image.to(device), label.to(device)\n",
    "      outputs = model(image)\n",
    "      probabilities, predicted = torch.max(outputs.data, 1)\n",
    "      val_loss += loss_function(outputs, label.long()).item()\n",
    "      total += label.size(0)\n",
    "      correct += (predicted == label).sum().item()\n",
    "      ground_truth_list += list(label.cpu())\n",
    "      predictions_list += list(predicted.cpu())\n",
    "\n",
    "  acc = sklearn.metrics.accuracy_score(ground_truth_list,predictions_list)\n",
    "  f1 = sklearn.metrics.f1_score(ground_truth_list,predictions_list,average = 'macro')\n",
    "  precision = sklearn.metrics.precision_score(ground_truth_list,predictions_list,average = 'macro')\n",
    "  recall = sklearn.metrics.recall_score(ground_truth_list,predictions_list,average = 'macro')\n",
    "  print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')\n",
    "\n",
    "  metrics_dict = {'val_loss':val_loss,'acc':acc,'f1':f1,'precision':precision,'recall':recall}\n",
    "\n",
    "  return metrics_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big-self supervised models are strong semi-supervised radiologists\n",
    "\n",
    "\n",
    "Inspired by [this paper](https://arxiv.org/pdf/2006.10029.pdf)\n",
    "\n",
    "\n",
    "We have a small subset of labeled data $L$ and a large pool of unlabeled data $U$. The goal is to make the most out of $U$ for training a classifier for solving the task on $L$.\n",
    "\n",
    "The procedure has three steps: \n",
    "\n",
    "* Pretrain a big SimCLR model on $U$\n",
    "* Fine-tune on $L$\n",
    "* Use the resulting model as a teacher for a smaller model, which is trained on the predictions of the teacher model on $U$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split $L$ into a training and testing set using stratified sampling for getting splits with the same proportions of labels. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results path\n",
    "results_path = Path('/projects/self_supervised/results/bw_caries_distillation')\n",
    "results_path.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "\n",
    "sample_unlabeled = 1.0\n",
    "\n",
    "data_path = Path('/projects/self_supervised/data/bitewings_caries/classification_dataset')\n",
    "unlabeled_path = Path('/projects/self_supervised/data/bitewings_caries/bw_U')\n",
    "labeled_path = Path('/projects/self_supervised/data/bitewings_caries/bw_L')\n",
    "train_path = Path('/projects/self_supervised/data/bitewings_caries/bw_train')\n",
    "test_path = Path('/projects/self_supervised/data/bitewings_caries/bw_test')  \n",
    "\n",
    "# #not required if you already splitted the data\n",
    "\n",
    "\n",
    "# make_train_test_split(\n",
    "#     labeled_path = data_path,\n",
    "#     train_path = unlabeled_path,\n",
    "#     test_path = labeled_path,\n",
    "#     test_size = 0.1\n",
    "# ) \n",
    "\n",
    "\n",
    "# make_train_test_split(\n",
    "#     labeled_path = labeled_path,\n",
    "#     train_path = train_path,\n",
    "#     test_path = test_path,\n",
    "#     test_size = 0.5\n",
    "# )  \n",
    "\n",
    "\n",
    "input_size = 128\n",
    "batch_size = 16\n",
    "num_workers = 2\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(360),\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "    mean=lightly.data.collate.imagenet_normalize['mean'],\n",
    "    std=lightly.data.collate.imagenet_normalize['std'],\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "    mean=lightly.data.collate.imagenet_normalize['mean'],\n",
    "    std=lightly.data.collate.imagenet_normalize['std'],\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "trainset = datasets.ImageFolder(root=train_path, transform=train_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "testset = datasets.ImageFolder(root=test_path, transform=test_transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "class_index_dict = {v:k for k,v in testset.class_to_idx.items()}\n",
    "\n",
    "#load unlabeled data\n",
    "\n",
    "unlabeled_dataset = datasets.ImageFolder(root=unlabeled_path, transform=train_transform)\n",
    "unlabeled_dataset = sample_dataset(unlabeled_dataset,sample_unlabeled)\n",
    "unlabeledloader = torch.utils.data.DataLoader(\n",
    "    unlabeled_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 192\n",
      "testing 193\n",
      "unlabeled 3457\n"
     ]
    }
   ],
   "source": [
    "print('training',len(trainset))\n",
    "#print('training',class_distribution(trainset,class_index_dict))\n",
    "print('testing',len(testset))\n",
    "#print('testing',class_distribution(testset,class_index_dict))\n",
    "print('unlabeled',len(unlabeled_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla student model (resnet18)\n",
    "\n",
    "We train on $L$ a resnet18 as a baseline.\n",
    "\n",
    "We define the softmax-like function $P(y|x_{i})=\\frac{exp(f(x_{i})[y]/\\tau}{\\sum_{y'} exp(f(x_{i})[y']/\\tau}$, where $\\tau$ is a temperature parameter and $f$ the model. \n",
    "\n",
    "We will use cross-entropy defined as $-\\sum_{(x_{i},y_{i})} \\left[ log P(y_{i}|x_{i})\\right]$ as the loss function for training on $L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(nn.Module):\n",
    "    def __init__(self,output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = models.resnet18(pretrained=True)\n",
    "        self.net.fc = nn.Linear(self.net.fc.in_features, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "    \n",
    "def P(x,tau = 1.0):\n",
    "  return torch.exp(x/tau)/(torch.exp(x/tau).sum())\n",
    "\n",
    "class CrossEntropyLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,n_categories):\n",
    "        super(CrossEntropyLoss,self).__init__()\n",
    "        self.n_classes = n_categories\n",
    "\n",
    "    def forward(self, prediction, label):\n",
    "      label = torch.nn.functional.one_hot(label,num_classes=n_categories)\n",
    "      loss = -label*torch.log(P(prediction))\n",
    "      return loss.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_categories = len([cat for cat in labeled_path.iterdir()])\n",
    "crossent_loss = CrossEntropyLoss(n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the student model on L+U\n",
    "\n",
    "def FolderDataset(data_path):\n",
    "    img_path_list = []\n",
    "    label_list = []\n",
    "    for label in data_path.iterdir():\n",
    "        for img in data_path.joinpath(label).iterdir():\n",
    "            label_list.append(label)\n",
    "            img_path_list.append(img)\n",
    "            \n",
    "    return np.array(img_path_list), np.array(label_list)\n",
    "    \n",
    "\n",
    "class FullDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs_path,labels, transform):\n",
    "   \n",
    "        self.imgs_path = imgs_path\n",
    "        self.labels = labels                \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = self.imgs_path[idx]\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        return tensor_image,label\n",
    "    \n",
    "    \n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "train_imgs,train_labels = FolderDataset(train_path) \n",
    "U_imgs,U_labels = FolderDataset(train_path) \n",
    "imgs = np.concatenate((train_imgs,U_imgs))\n",
    "labels = np.concatenate((train_labels,U_labels))\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "full_dataset = FullDataset(imgs,labels,train_transform)\n",
    "\n",
    "full_dataloader = torch.utils.data.DataLoader(\n",
    "    full_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.698 f1:0.515 precision:0.595 recall:0.537\n",
      "acc:0.693 f1:0.521 precision:0.585 recall:0.538\n",
      "acc:0.667 f1:0.536 precision:0.559 recall:0.540\n",
      "acc:0.719 f1:0.548 precision:0.676 recall:0.564\n",
      "acc:0.667 f1:0.521 precision:0.553 recall:0.531\n",
      "acc:0.646 f1:0.442 precision:0.456 recall:0.482\n",
      "acc:0.672 f1:0.488 precision:0.537 recall:0.516\n",
      "\n",
      "Best metrics:\n",
      "acc:0.667 f1:0.536 precision:0.559 recall:0.540\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "student = Student(n_categories).to(device)\n",
    "optimizer = optim.Adam(student.parameters(), lr=0.0001)\n",
    "\n",
    "patience = 3\n",
    "count = 0\n",
    "best_loss = 1e9\n",
    "for epoch in range(40):\n",
    "  for image,label in full_dataloader:\n",
    "      image, label = image.to(device), label.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      loss = crossent_loss(student(image), label)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "        \n",
    "  metrics_dict = evaluate(student,testloader,crossent_loss)\n",
    "  val_loss = metrics_dict['val_loss']\n",
    "  if val_loss < best_loss:\n",
    "        #torch.save(student.state_dict(),results_path.joinpath('student.pth'))\n",
    "        best_loss = val_loss\n",
    "        best_metrics = metrics_dict\n",
    "        \n",
    "        count = 0\n",
    "  else:\n",
    "    count += 1\n",
    "  if count > patience:\n",
    "    break\n",
    "        \n",
    "        \n",
    "print('\\nBest metrics:')\n",
    "acc = best_metrics['acc']\n",
    "f1 = best_metrics['f1']\n",
    "recall = best_metrics['recall']\n",
    "precision = best_metrics['precision']\n",
    "print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.630 f1:0.481 precision:0.493 recall:0.495\n",
      "acc:0.651 f1:0.466 precision:0.491 recall:0.496\n",
      "acc:0.661 f1:0.461 precision:0.496 recall:0.498\n",
      "acc:0.646 f1:0.463 precision:0.483 recall:0.492\n",
      "acc:0.667 f1:0.504 precision:0.542 recall:0.522\n",
      "\n",
      "Best metrics:\n",
      "acc:0.630 f1:0.481 precision:0.493 recall:0.495\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train the student model on L\n",
    "\n",
    "student = Student(n_categories).to(device)\n",
    "optimizer = optim.Adam(student.parameters(), lr=0.0001)\n",
    "\n",
    "patience = 3\n",
    "count = 0\n",
    "best_loss = 1e9\n",
    "for epoch in range(40):\n",
    "  for image,label in trainloader:\n",
    "      image, label = image.to(device), label.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      loss = crossent_loss(student(image), label)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "        \n",
    "  metrics_dict = evaluate(student,testloader,crossent_loss)\n",
    "  val_loss = metrics_dict['val_loss']\n",
    "  if val_loss < best_loss:\n",
    "        torch.save(student.state_dict(),results_path.joinpath('student.pth'))\n",
    "        best_loss = val_loss\n",
    "        best_metrics = metrics_dict\n",
    "        \n",
    "        count = 0\n",
    "  else:\n",
    "    count += 1\n",
    "  if count > patience:\n",
    "    break\n",
    "        \n",
    "        \n",
    "print('\\nBest metrics:')\n",
    "acc = best_metrics['acc']\n",
    "f1 = best_metrics['f1']\n",
    "recall = best_metrics['recall']\n",
    "precision = best_metrics['precision']\n",
    "print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big teacher (SimCLR)\n",
    "\n",
    "\n",
    "We fine-tune on $L$ a SimCLR model with backbone resnet50 pretrained on $U$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(nn.Module):\n",
    "    def __init__(self, model,num_ftrs,output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.freeze = False\n",
    "        self.net = model\n",
    "        self.fc1 = nn.Linear(num_ftrs, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.freeze:\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net.backbone(x).squeeze()\n",
    "                y_hat = self.fc1(y_hat)\n",
    "                y_hat = self.relu(y_hat)\n",
    "                y_hat = self.fc2(y_hat)\n",
    "                return y_hat\n",
    "        else:\n",
    "            y_hat = self.net.backbone(x).squeeze()\n",
    "            y_hat = self.fc1(y_hat)\n",
    "            y_hat = self.relu(y_hat)\n",
    "            y_hat = self.fc2(y_hat)\n",
    "            return y_hat\n",
    "            \n",
    "    def freeze_weights(self):\n",
    "      for p in self.net.parameters():\n",
    "          p.requires_grad = False\n",
    "      self.freeze = True\n",
    "      return self\n",
    "\n",
    "    def unfreeze_weights(self):\n",
    "      for p in self.net.parameters():\n",
    "          p.requires_grad = True\n",
    "      self.freeze = False\n",
    "      return self\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.490 f1:0.459 precision:0.475 recall:0.470\n",
      "acc:0.490 f1:0.452 precision:0.464 recall:0.458\n",
      "acc:0.505 f1:0.468 precision:0.478 recall:0.474\n",
      "acc:0.500 f1:0.454 precision:0.462 recall:0.456\n",
      "acc:0.521 f1:0.476 precision:0.483 recall:0.481\n",
      "acc:0.547 f1:0.492 precision:0.495 recall:0.494\n",
      "acc:0.583 f1:0.515 precision:0.515 recall:0.516\n",
      "acc:0.542 f1:0.433 precision:0.431 recall:0.437\n",
      "acc:0.547 f1:0.460 precision:0.460 recall:0.460\n",
      "acc:0.562 f1:0.465 precision:0.464 recall:0.467\n",
      "acc:0.630 f1:0.529 precision:0.534 recall:0.529\n",
      "acc:0.568 f1:0.442 precision:0.440 recall:0.449\n",
      "acc:0.589 f1:0.469 precision:0.470 recall:0.474\n",
      "acc:0.583 f1:0.459 precision:0.459 recall:0.467\n",
      "acc:0.583 f1:0.452 precision:0.451 recall:0.462\n",
      "acc:0.589 f1:0.440 precision:0.438 recall:0.456\n",
      "acc:0.604 f1:0.465 precision:0.468 recall:0.477\n",
      "acc:0.589 f1:0.448 precision:0.447 recall:0.461\n",
      "acc:0.620 f1:0.466 precision:0.474 recall:0.483\n",
      "acc:0.594 f1:0.425 precision:0.419 recall:0.448\n",
      "acc:0.625 f1:0.469 precision:0.479 recall:0.487\n",
      "acc:0.615 f1:0.426 precision:0.423 recall:0.460\n",
      "acc:0.609 f1:0.452 precision:0.453 recall:0.469\n",
      "acc:0.651 f1:0.494 precision:0.514 recall:0.509\n",
      "acc:0.615 f1:0.471 precision:0.477 recall:0.484\n",
      "acc:0.630 f1:0.464 precision:0.475 recall:0.486\n",
      "acc:0.625 f1:0.469 precision:0.477 recall:0.485\n",
      "acc:0.625 f1:0.469 precision:0.479 recall:0.487\n",
      "acc:0.625 f1:0.461 precision:0.470 recall:0.482\n",
      "acc:0.677 f1:0.536 precision:0.574 recall:0.544\n",
      "acc:0.672 f1:0.524 precision:0.561 recall:0.535\n",
      "acc:0.656 f1:0.497 precision:0.525 recall:0.514\n",
      "acc:0.641 f1:0.479 precision:0.497 recall:0.498\n",
      "acc:0.646 f1:0.507 precision:0.525 recall:0.516\n",
      "acc:0.677 f1:0.519 precision:0.565 recall:0.534\n",
      "acc:0.656 f1:0.506 precision:0.528 recall:0.517\n",
      "acc:0.625 f1:0.469 precision:0.479 recall:0.487\n",
      "acc:0.651 f1:0.510 precision:0.531 recall:0.520\n",
      "acc:0.656 f1:0.521 precision:0.544 recall:0.529\n",
      "acc:0.656 f1:0.488 precision:0.518 recall:0.509\n",
      "\n",
      "Best metrics:\n",
      "acc:0.656 f1:0.521 precision:0.544 recall:0.529\n"
     ]
    }
   ],
   "source": [
    "#finetune simclr\n",
    "simclr_results_path = Path('/projects/self_supervised/results/bitewings_caries_moco_50')\n",
    "\n",
    "teacher = load_simcrl(simclr_results_path,n_categories,model_size = 50)\n",
    "optimizer = optim.Adam(teacher.parameters(), lr=0.000005)\n",
    "\n",
    "patience = 3\n",
    "count = 0\n",
    "best_loss = 1e9\n",
    "for epoch in range(40):\n",
    "  for image,label in trainloader:\n",
    "      image, label = image.to(device), label.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      loss = crossent_loss(teacher(image), label.long())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "  metrics_dict = evaluate(teacher,testloader,crossent_loss)\n",
    "  val_loss = metrics_dict['val_loss']\n",
    "  if val_loss < best_loss:\n",
    "        torch.save(teacher.state_dict(),results_path.joinpath('teacher.pth'))\n",
    "        best_loss = val_loss\n",
    "        best_metrics = metrics_dict\n",
    "        count = 0\n",
    "        \n",
    "  else:\n",
    "    count += 1\n",
    "  if count > patience:\n",
    "    break\n",
    "\n",
    "print('\\nBest metrics:')\n",
    "acc = best_metrics['acc']\n",
    "f1 = best_metrics['f1']\n",
    "recall = best_metrics['recall']\n",
    "precision = best_metrics['precision']\n",
    "print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model distillation\n",
    "\n",
    "The fine-tuned model often yields better performance than the vanilla model. Now that we used $L$ for finetuning the teacher model, we can use $U$ again for transfering the knowledge from the teacher to the student\n",
    "\n",
    "The procedure is as follows:\n",
    "* sample data from $U$\n",
    "* predict with the teacher\n",
    "* use the labels as targets for training the student\n",
    "\n",
    "This algorithm is expected to yield an even better model. Notice that the resulting model would be a resnet18 with even better performance than a resnet50 pretrained on $U$ and fine-tuned on $L$.\n",
    "\n",
    "We will use a distillation loss, which takes the output probabilities of the teacher model as the target for the student model.\n",
    "\n",
    "$$ -\\sum_{x_{i}} \\left[ \\sum_{y} P^{T}(y|x_{i};\\tau) log P^{S}(y|x_{i};\\tau) \\right] $$\n",
    "\n",
    "With this loss function the student doesn't only see the one-vs-zero encoding used in supervised learning, but a probability distribution as a target. This could help the student model to learn nuances of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistilLoss,self).__init__()\n",
    "\n",
    "    def forward(self, t_output, s_output):\n",
    "      loss = -P(t_output)*torch.log(P(s_output))\n",
    "      return loss.sum().sum()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.646 f1:0.580 precision:0.580 recall:0.580\n",
      "acc:0.688 f1:0.508 precision:0.569 recall:0.529\n",
      "acc:0.703 f1:0.413 precision:0.352 recall:0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/miniconda3/envs/sort_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.698 f1:0.411 precision:0.349 recall:0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/miniconda3/envs/sort_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.698 f1:0.411 precision:0.349 recall:0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/miniconda3/envs/sort_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.693 f1:0.409 precision:0.348 recall:0.496\n",
      "acc:0.693 f1:0.425 precision:0.516 recall:0.501\n",
      "acc:0.698 f1:0.411 precision:0.349 recall:0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/miniconda3/envs/sort_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.703 f1:0.429 precision:0.851 recall:0.509\n",
      "acc:0.698 f1:0.411 precision:0.349 recall:0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/miniconda3/envs/sort_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.703 f1:0.413 precision:0.352 recall:0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/miniconda3/envs/sort_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.703 f1:0.445 precision:0.685 recall:0.514\n",
      "acc:0.698 f1:0.411 precision:0.349 recall:0.500\n",
      "\n",
      "Best metrics:\n",
      "acc:0.693 f1:0.425 precision:0.516 recall:0.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/miniconda3/envs/sort_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#init student\n",
    "student = Student(n_categories).to(device)\n",
    "\n",
    "#load teacher checkpoint\n",
    "teacher = load_simcrl(simclr_results_path,n_categories,model_size = 50)\n",
    "teacher.load_state_dict(torch.load(results_path.joinpath('teacher.pth')))\n",
    "teacher.eval()\n",
    "\n",
    "distill_loss = DistilLoss()\n",
    "optimizer = optim.Adam(student.parameters(), lr=0.00001)\n",
    "\n",
    "patience = 5\n",
    "count = 0\n",
    "best_loss = 1e9\n",
    "for epoch in range(40):\n",
    "    \n",
    "#   for image,label in trainloader:\n",
    "#       image, label = image.to(device), label.to(device)\n",
    "#       optimizer.zero_grad()\n",
    "#       loss = crossent_loss(student(image), label)\n",
    "#       loss.backward()\n",
    "#       optimizer.step()\n",
    "        \n",
    "  for image,_ in unlabeledloader:\n",
    "    image = image.to(device)\n",
    "    loss = distill_loss(teacher(image),student(image))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  metrics_dict = evaluate(student,testloader,crossent_loss)\n",
    "  val_loss = metrics_dict['val_loss']\n",
    "  if val_loss < best_loss:        \n",
    "        best_loss = val_loss\n",
    "        best_metrics = metrics_dict\n",
    "        count = 0\n",
    "  else:\n",
    "    count += 1\n",
    "  if count > patience:\n",
    "    break\n",
    "\n",
    "print('\\nBest metrics:')\n",
    "acc = best_metrics['acc']\n",
    "f1 = best_metrics['f1']\n",
    "recall = best_metrics['recall']\n",
    "precision = best_metrics['precision']\n",
    "print(f'acc:{acc:.3f} f1:{f1:.3f} precision:{precision:.3f} recall:{recall:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
